# ChatGLM2å¾®è°ƒæŒ‡å— ğŸ’¡

## 1. ç¯å¢ƒæ­å»º
### å®‰è£…ä¾èµ–
```
git clone https://github.com/THUDM/ChatGLM2-6B
cd ChatGLM2-6B
pip install -r requirements.txt
pip install rouge_chinese nltk jieba datasets
```
### ä¸‹è½½æ¨¡å‹
```
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
sudo apt install build-essential
git clone https://huggingface.co/THUDM/chatglm2-6b
```

## 2. å‡†å¤‡æ•°æ®é›†

æ•°æ®é›†æ ¼å¼å‚è€ƒï¼š
```json lines
{"prompt": "é•¿åŸh3é£æ‰‡ä¸è½¬ã€‚ç»§ç”µå™¨å¥½çš„ã€‚ä¿é™©ä¸å¥½çš„ä¼ æ„Ÿå™¨æ–°çš„é£æ‰‡ä¹Ÿæ–°çš„è¿™æ˜¯ä¸ºä»€ä¹ˆã€‚å°±æ˜¯ç»§ç”µå™¨ç¼ºä¸€ä¸ªä¿¡å·çº¿", "response": "ç”¨ç”µè„‘èƒ½è¯»æ•°æ®æµå—ï¼Ÿæ°´æ¸©å¤šå°‘", "history": []}
{"prompt": "95", "response": "ä¸Šä¸‹æ°´ç®¡æ¸©å·®æ€ä¹ˆæ ·å•Šï¼Ÿç©ºæ°”æ˜¯ä¸æ˜¯éƒ½æ’å¹²å‡€äº†å‘¢ï¼Ÿ", "history": [["é•¿åŸh3é£æ‰‡ä¸è½¬ã€‚ç»§ç”µå™¨å¥½çš„ã€‚ä¿é™©ä¸å¥½çš„ä¼ æ„Ÿå™¨æ–°çš„é£æ‰‡ä¹Ÿæ–°çš„è¿™æ˜¯ä¸ºä»€ä¹ˆã€‚å°±æ˜¯ç»§ç”µå™¨ç¼ºä¸€ä¸ªä¿¡å·çº¿", "ç”¨ç”µè„‘èƒ½è¯»æ•°æ®æµå—ï¼Ÿæ°´æ¸©å¤šå°‘"]]}
{"prompt": "æ˜¯çš„ã€‚ä¸Šä¸‹æ°´ç®¡éƒ½å¥½çš„", "response": "é‚£å°±è¦æ£€æŸ¥çº¿è·¯äº†ï¼Œä¸€èˆ¬é£æ‰‡ç»§ç”µå™¨æ˜¯ç”±ç”µè„‘æ§åˆ¶å¸åˆçš„ï¼Œå¦‚æœç”µè·¯å­˜åœ¨æ–­è·¯ï¼Œæˆ–è€…ç”µè„‘åäº†çš„è¯ä¼šå‡ºç°ç»§ç”µå™¨ä¸å¸åˆçš„æƒ…å†µï¼", "history": [["é•¿åŸh3é£æ‰‡ä¸è½¬ã€‚ç»§ç”µå™¨å¥½çš„ã€‚ä¿é™©ä¸å¥½çš„ä¼ æ„Ÿå™¨æ–°çš„é£æ‰‡ä¹Ÿæ–°çš„è¿™æ˜¯ä¸ºä»€ä¹ˆã€‚å°±æ˜¯ç»§ç”µå™¨ç¼ºä¸€ä¸ªä¿¡å·çº¿", "ç”¨ç”µè„‘èƒ½è¯»æ•°æ®æµå—ï¼Ÿæ°´æ¸©å¤šå°‘"], ["95", "ä¸Šä¸‹æ°´ç®¡æ¸©å·®æ€ä¹ˆæ ·å•Šï¼Ÿç©ºæ°”æ˜¯ä¸æ˜¯éƒ½æ’å¹²å‡€äº†å‘¢ï¼Ÿ"]]}
```
åˆ†åˆ«å‡†å¤‡è®­ç»ƒæ•°æ®é›† `train.json` å’ŒéªŒè¯æ•°æ®é›† `dev.json` å¹¶å°†å…¶ä¸Šä¼ è‡³ `ChatGLM2-6B` æ–‡ä»¶å¤¹ä¸‹

## 3. å¼€å§‹è®­ç»ƒ

åœ¨ç»ˆç«¯è¿è¡ŒæŒ‡ä»¤
```shell
bash train_chat.sh
```
å³å¯å¼€å§‹è®­ç»ƒ

åŸ `train_chat.sh` æ–‡ä»¶ä¸­åŒ…å«ä»¥ä¸‹ä»£ç ï¼š
```
PRE_SEQ_LEN=128
LR=1e-2
NUM_GPUS=1

torchrun --standalone --nnodes=1 --nproc-per-node=$NUM_GPUS main.py \
    --do_train \
    --train_file $CHAT_TRAIN_DATA \
    --validation_file $CHAT_VAL_DATA \
    --preprocessing_num_workers 10 \
    --prompt_column prompt \
    --response_column response \
    --history_column history \
    --overwrite_cache \
    --model_name_or_path THUDM/chatglm2-6b \
    --output_dir $CHECKPOINT_NAME \
    --overwrite_output_dir \
    --max_source_length 256 \
    --max_target_length 256 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --predict_with_generate \
    --max_steps 3000 \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate $LR \
    --pre_seq_len $PRE_SEQ_LEN \
    --quantization_bit 4
```
åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œéœ€è¦å°†å…¶ç¼–è¾‘ä¸ºä»¥ä¸‹ä»£ç ï¼š
```
PRE_SEQ_LEN=128
LR=1e-2
NUM_GPUS=1

torchrun --standalone --nnodes=1 --nproc-per-node=$NUM_GPUS ptuning/main.py \
    --do_train \
    --train_file train.json \
    --validation_file dev.json \
    --preprocessing_num_workers 10 \
    --prompt_column prompt \
    --response_column response \
    --history_column history \
    --overwrite_cache \
    --model_name_or_path chatglm2-6b \
    --output_dir output_model \
    --overwrite_output_dir \
    --max_source_length 1024 \
    --max_target_length 1024 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --predict_with_generate \
    --max_steps 600 \
    --logging_steps 10 \
    --save_steps 100 \
    --learning_rate $LR \
    --pre_seq_len $PRE_SEQ_LEN
```

P.S. ä»¥ä¸Šçš„ `train_chat.sh` æ–‡ä»¶åªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå…·ä½“å‚æ•°è®¾ç½®éœ€è¦æ ¹æ®ä¸åŒæ˜¾å¡çš„æ€§èƒ½è¿›è¡Œè°ƒèŠ‚ï¼›ChatGLM2å¾®è°ƒ[å®˜æ–¹æ•™ç¨‹](https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning)
